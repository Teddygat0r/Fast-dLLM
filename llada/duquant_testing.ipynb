{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc4a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuaz/dllm/Fast-dLLM/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model.modeling_llada import LLaDAModelLM\n",
    "from transformers import AutoTokenizer\n",
    "from quantization_calibration_dataset import LLaDACalibrationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from duquant_utils import create_quant_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0fd18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 707.20it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LLaDAModelLM.from_pretrained(MODEL_PATH, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fafa0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_args = {\n",
    "    \"nsamples\": 128,\n",
    "    \"seqlen\": 2048,\n",
    "    \"wbits\": 8,\n",
    "    \"abits\": 8,\n",
    "    \"alpha\": 0.5, # for smoothquant\n",
    "    \"act_group_size\": None,\n",
    "    \"smooth\": True,\n",
    "    \"quant_method\": \"duquant\",\n",
    "    \"symmetric\": True,\n",
    "    \"group_size\": None,\n",
    "    \"swc\": 0.8,\n",
    "    \"lac\": 0.9,\n",
    "    \"lwc\": False,\n",
    "    \"block_size\": 128,\n",
    "    \"max_rotation_step\": 256,\n",
    "    \"permutation_times\": 1,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "\n",
    "# args = argparse.Namespace(**user_args)\n",
    "args = create_quant_args(user_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bde2e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Calibration Buffer with Mask ID: 126336...\n",
      "Concatenating and tokenizing dataset...\n",
      "Total tokens in concatenated dataset: 2608998\n",
      "Calibration Dataset Ready: 128 tensors.\n"
     ]
    }
   ],
   "source": [
    "act_scales = torch.load(\"act_scales/LLaDA-8B-Instruct.pt\")        \n",
    "dataset = LLaDACalibrationDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    seq_len=args.seqlen,\n",
    "    samples=args.nsamples,\n",
    "    block_size=args.block_size,\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b5241fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "from duquant_utils import set_init_duquant_params_state, set_quant_state, smooth_and_let_inplace\n",
    "from model.quantize.int_linear import QuantLinear\n",
    "from model.int_llada_layer import LLaDaQuantLayer\n",
    "\n",
    "CLIPMIN = 1e-5\n",
    "\n",
    "def duquant(model: nn.Module, act_scales: dict, dataloader, args):\n",
    "    layers = model.model.transformer.blocks\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    dtype = torch.bfloat16\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seqlen = args.seqlen\n",
    "    pairs = {\n",
    "        \"q_proj\":\"qkv\",\n",
    "        \"attn_out\":\"out\",\n",
    "        \"up_proj\":\"fc1\",\n",
    "        \"ff_out\":\"down\",\n",
    "    }\n",
    "\n",
    "    inps = torch.zeros(\n",
    "        (args.nsamples, seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
    "    )\n",
    "    cache = {\"i\": 0}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "\n",
    "        def forward(self, inp, **kwargs):\n",
    "            if len(inp.shape) == 3:\n",
    "                 inps[cache[\"i\"]] = inp[0] \n",
    "            else:\n",
    "                 inps[cache[\"i\"]] = inp\n",
    "            cache[\"i\"] += 1\n",
    "            return self.module(inp, **kwargs)\n",
    "    \n",
    "    layers[0] = Catcher(layers[0])\n",
    "\n",
    "    input_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if cache[\"i\"] >= args.nsamples:\n",
    "                break\n",
    "            try:\n",
    "                input_ids.append(batch['input_ids'])\n",
    "                model(batch['input_ids'].to(dev))\n",
    "\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    layers[0] = layers[0].module\n",
    "    layers[0] = layers[0].cpu()\n",
    "\n",
    "    print(layers[0])\n",
    "    duquant_parameters = {}\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    quant_inps = inps\n",
    "    rotate_inps = copy.copy(inps).mean(dim=0)\n",
    "\n",
    "    fp_inps = copy.deepcopy(inps)\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "        print(\"Starting Layer, \" + str(i))\n",
    "        args.q_quant_params = copy.copy(args.act_quant_params)\n",
    "        args.k_quant_params = copy.copy(args.act_quant_params)\n",
    "        layer = layers[i]\n",
    "        qlayer = LLaDaQuantLayer(layer, args)\n",
    "        qlayer.set_quant_state(weight_quant=False, act_quant=True)\n",
    "\n",
    "        for name, module in layer.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                weight_quant = QuantLinear(module, weight_quant_params=copy.copy(args.weight_quant_params), act_quant_params=copy.copy(args.act_quant_params))\n",
    "                setattr(qlayer, name, weight_quant)\n",
    "\n",
    "        qlayer.load_state_dict(layer.state_dict())\n",
    "        qlayer.to(dev)\n",
    "\n",
    "        set_init_duquant_params_state(qlayer, True)\n",
    "        set_quant_state(qlayer, weight_quant=False, act_quant=True)\n",
    "\n",
    "        qlayer.register_parameter(\"qkt_smooth_scale\",torch.nn.Parameter(torch.ones(qlayer.q_proj.out_features,device=dev, dtype=dtype), requires_grad=False))\n",
    "        for name, module in qlayer.named_modules():\n",
    "            if isinstance(module, QuantLinear):\n",
    "                for key in pairs.keys():\n",
    "                    if key in name:\n",
    "                        act = act_scales[f\"model.transformer.blocks.{i}.{key}\"].to(device=dev, dtype=dtype).clamp(min=CLIPMIN)\n",
    "                        weight = module.weight.abs().max(dim=0)[0].clamp(min=CLIPMIN)\n",
    "                        scale = (act.pow(args.alpha)/weight.to(act.device).pow(1-args.alpha)).clamp(min=CLIPMIN)\n",
    "\n",
    "                        qlayer.register_parameter(f\"{pairs[key]}_smooth_scale\",torch.nn.Parameter(scale, requires_grad=False))\n",
    "\n",
    "        qlayer.to(dtype=torch.bfloat16)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                qlayer.qkt_smooth_scale.clamp_(min=0.5)\n",
    "        except:\n",
    "            pass\n",
    "        smooth_and_let_inplace(qlayer, args)\n",
    "\n",
    "        # perform duquant process\n",
    "        set_init_duquant_params_state(qlayer, False)\n",
    "        set_quant_state(qlayer, weight_quant=True, act_quant=True)\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type=dev):\n",
    "                rotate_inps = qlayer(rotate_inps.unsqueeze(0))[0][0]\n",
    "            qlayer.register_duquant_params()\n",
    "            set_init_duquant_params_state(qlayer, True)\n",
    "\n",
    "        qlayer.to(dtype=torch.bfloat16)\n",
    "        with torch.no_grad():\n",
    "            for name, module in qlayer.named_modules():\n",
    "                if isinstance(module, QuantLinear):\n",
    "                    module.weight = module.weight_quantizer(module.weight, return_no_quant=True)\n",
    "\n",
    "        set_quant_state(qlayer, weight_quant=False, act_quant=True)\n",
    "        layers[i] = qlayer.to(\"cpu\")\n",
    "        # i dont think this is necessary for loading\n",
    "        # duquant_parameters[i] = duquant_state_dict(qlayer)\n",
    "\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.model.transformer.embed_tokens = model.model.transformer.wte.to('cpu')\n",
    "    del inps\n",
    "    del quant_inps\n",
    "    del fp_inps\n",
    "    del rotate_inps\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()                    \n",
    "    model.config.use_cache = use_cache\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "950f97cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaDALlamaBlock(\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act): SiLU()\n",
      "  (attn_out): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (ff_out): Linear(in_features=12288, out_features=4096, bias=False)\n",
      "  (rotary_emb): RotaryEmbedding()\n",
      "  (attn_norm): RMSLayerNorm()\n",
      "  (ff_norm): RMSLayerNorm()\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (ff_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
      ")\n",
      "Starting Layer, 0\n",
      "Starting Layer, 1\n",
      "Starting Layer, 2\n",
      "Starting Layer, 3\n",
      "Starting Layer, 4\n",
      "Starting Layer, 5\n",
      "Starting Layer, 6\n",
      "Starting Layer, 7\n",
      "Starting Layer, 8\n",
      "Starting Layer, 9\n",
      "Starting Layer, 10\n",
      "Starting Layer, 11\n",
      "Starting Layer, 12\n",
      "Starting Layer, 13\n",
      "Starting Layer, 14\n",
      "Starting Layer, 15\n",
      "Starting Layer, 16\n",
      "Starting Layer, 17\n",
      "Starting Layer, 18\n",
      "Starting Layer, 19\n",
      "Starting Layer, 20\n",
      "Starting Layer, 21\n",
      "Starting Layer, 22\n",
      "Starting Layer, 23\n",
      "Starting Layer, 24\n",
      "Starting Layer, 25\n",
      "Starting Layer, 26\n",
      "Starting Layer, 27\n",
      "Starting Layer, 28\n",
      "Starting Layer, 29\n",
      "Starting Layer, 30\n",
      "Starting Layer, 31\n"
     ]
    }
   ],
   "source": [
    "model = duquant(model, act_scales, dataloader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6055bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaDAModelLM(\n",
       "  (model): LLaDAModel(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(126464, 4096)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): RMSLayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x LLaDaQuantLayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SiLU()\n",
       "          (attn_out): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (ff_out): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): RMSLayerNorm()\n",
       "          (ff_norm): RMSLayerNorm()\n",
       "          (q_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (ff_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (qkt_matmul): QuantMatMul(\n",
       "            (x1_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (x2_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (pv_matmul): QuantMatMul(\n",
       "            (x1_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (x2_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ff_out): Linear(in_features=4096, out_features=126464, bias=False)\n",
       "      (embed_tokens): Embedding(126464, 4096)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33468b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot's reply: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from generate import generate\n",
    "\n",
    "user_input = input(\"Enter your question: \")\n",
    "\n",
    "m = [{\"role\": \"user\", \"content\": user_input}]\n",
    "user_input = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "input_ids = tokenizer(user_input)['input_ids']\n",
    "input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "\n",
    "out, nfe = generate(model, input_ids, steps=128, gen_length=128, block_length=args.block_size, temperature=0., remasking='low_confidence', threshold=0.9)\n",
    "answer = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"Bot's reply: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "856fbd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 213.11it/s]\n"
     ]
    }
   ],
   "source": [
    "original_model = LLaDAModelLM.from_pretrained(MODEL_PATH, trust_remote_code=True, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43d03495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equality found in model.transformer.ff_out.weight\n",
      "Quantized weight shape: torch.Size([126464, 4096])\n",
      "Original weight shape: torch.Size([126464, 4096])\n",
      "Equality found in model.transformer.wte.weight\n",
      "Quantized weight shape: torch.Size([126464, 4096])\n",
      "Original weight shape: torch.Size([126464, 4096])\n",
      "Equality found in model.transformer.ln_f.weight\n",
      "Quantized weight shape: torch.Size([4096])\n",
      "Original weight shape: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "quantized_state_dict = model.state_dict()\n",
    "original_state_dict = original_model.state_dict()\n",
    "\n",
    "for key in quantized_state_dict.keys():\n",
    "    quantized_state_dict[key] = quantized_state_dict[key].to(\"cpu\")\n",
    "\n",
    "for key in original_state_dict.keys():\n",
    "    original_state_dict[key] = original_state_dict[key].to(\"cpu\")\n",
    "\n",
    "common_keys = set(quantized_state_dict.keys()) & set(original_state_dict.keys())\n",
    "for key in common_keys:\n",
    "    quantized_weight = quantized_state_dict[key]\n",
    "    original_weight = original_state_dict[key]\n",
    "    if torch.equal(quantized_weight, original_weight):\n",
    "        print(f\"Equality found in {key}\")\n",
    "        print(f\"Quantized weight shape: {quantized_weight.shape}\")\n",
    "        print(f\"Original weight shape: {original_weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d7ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

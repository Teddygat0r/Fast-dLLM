{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df32f1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuaz/dllm/Fast-dLLM/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from duquant_utils import *\n",
    "from model.modeling_llada import LLaDAModelLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69670b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "QUANT_ARGS_PATH = \"model/quantize/quant_args.json\"\n",
    "QUANTIZED_WEIGHTS_PATH = \"models/quantized_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32406016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 687.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LLaDAModelLM.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map=\"cpu\", dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "quant_config = json.load(open(QUANT_ARGS_PATH))\n",
    "model_weights = torch.load(QUANTIZED_WEIGHTS_PATH, map_location=\"cpu\")\n",
    "\n",
    "quant_args = create_quant_args(quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289b7cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuaz/dllm/Fast-dLLM/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:284: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:33<00:00,  5.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLaDAModelLM(\n",
       "  (model): LLaDAModel(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(126464, 4096)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): RMSLayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x LLaDALlamaBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SiLU()\n",
       "          (attn_out): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (ff_out): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): RMSLayerNorm()\n",
       "          (ff_norm): RMSLayerNorm()\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (ff_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff_out): Linear(in_features=4096, out_features=126464, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model = LLaDAModelLM.from_pretrained(MODEL_PATH, trust_remote_code=True, device_map=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "original_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d1019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing block model.transformer.blocks.0\n",
      "Replacing block model.transformer.blocks.1\n",
      "Replacing block model.transformer.blocks.2\n",
      "Replacing block model.transformer.blocks.3\n",
      "Replacing block model.transformer.blocks.4\n",
      "Replacing block model.transformer.blocks.5\n",
      "Replacing block model.transformer.blocks.6\n",
      "Replacing block model.transformer.blocks.7\n",
      "Replacing block model.transformer.blocks.8\n",
      "Replacing block model.transformer.blocks.9\n",
      "Replacing block model.transformer.blocks.10\n",
      "Replacing block model.transformer.blocks.11\n",
      "Replacing block model.transformer.blocks.12\n",
      "Replacing block model.transformer.blocks.13\n",
      "Replacing block model.transformer.blocks.14\n",
      "Replacing block model.transformer.blocks.15\n",
      "Replacing block model.transformer.blocks.16\n",
      "Replacing block model.transformer.blocks.17\n",
      "Replacing block model.transformer.blocks.18\n",
      "Replacing block model.transformer.blocks.19\n",
      "Replacing block model.transformer.blocks.20\n",
      "Replacing block model.transformer.blocks.21\n",
      "Replacing block model.transformer.blocks.22\n",
      "Replacing block model.transformer.blocks.23\n",
      "Replacing block model.transformer.blocks.24\n",
      "Replacing block model.transformer.blocks.25\n",
      "Replacing block model.transformer.blocks.26\n",
      "Replacing block model.transformer.blocks.27\n",
      "Replacing block model.transformer.blocks.28\n",
      "Replacing block model.transformer.blocks.29\n",
      "Replacing block model.transformer.blocks.30\n",
      "Replacing block model.transformer.blocks.31\n",
      "Loading Quantized Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLaDAModelLM(\n",
       "  (model): LLaDAModel(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(126464, 4096)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): RMSLayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x LLaDaQuantLayer(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SiLU()\n",
       "          (attn_out): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (ff_out): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): RMSLayerNorm()\n",
       "          (ff_norm): RMSLayerNorm()\n",
       "          (q_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (ff_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            (weight_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (act_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (qkt_matmul): QuantMatMul(\n",
       "            (x1_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (x2_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (pv_matmul): QuantMatMul(\n",
       "            (x1_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (x2_quantizer): UniformAffineQuantizer(\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ff_out): Linear(in_features=4096, out_features=126464, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading duquant model:\n",
    "\n",
    "replace_llada_blocks(model, quant_args, device=\"cpu\")\n",
    "\n",
    "replace_linear_layers(model, quant_args, model_weights)\n",
    "\n",
    "print(\"Loading Quantized Model...\")\n",
    "missing_keys, unexpected_keys = model.load_state_dict(model_weights, strict=False)\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0596e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.quantize.int_linear import QuantLinear\n",
    "\n",
    "def create_activation_hook(storage, name):\n",
    "    def hook(module, input, output):\n",
    "        x = input[0] if isinstance(input, tuple) else input\n",
    "        storage[name] = x.detach().cpu().clone()\n",
    "    return hook\n",
    "\n",
    "def register_activation_hooks(model, activations):\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, QuantLinear):\n",
    "            hook = module.register_forward_hook(create_activation_hook(activations, name))\n",
    "            hooks.append(hook)\n",
    "    return hooks\n",
    "\n",
    "def remove_hooks(hooks):\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    print(f\"Removed {len(hooks)} hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d09b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input shape: torch.Size([1, 1024])\n",
      "Number of masked tokens: 512 / 1024 (50%)\n"
     ]
    }
   ],
   "source": [
    "# Create test input using a sample prompt\n",
    "# For LLaDA, we also apply some masking to simulate the diffusion process\n",
    "\n",
    "test_prompt = \"\"\"The quick brown fox jumps over the lazy dog. This is a test sentence \n",
    "to analyze how SmoothQuant affects the activation distributions in the LLaDA model.\n",
    "Machine learning models often have outlier activations that make quantization difficult.\n",
    "SmoothQuant addresses this by migrating quantization difficulty from activations to weights.\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "encoded = tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    max_length=1024,\n",
    "    truncation=True,\n",
    ")\n",
    "test_input_ids = encoded.input_ids.to(\"cuda\")\n",
    "\n",
    "# Get the mask token id for LLaDA\n",
    "if hasattr(tokenizer, \"mask_token_id\") and tokenizer.mask_token_id is not None:\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "else:\n",
    "    mask_token_id = 126336  # Default for LLaDA\n",
    "\n",
    "# Apply partial masking to simulate LLaDA's input (50% masking)\n",
    "mask_ratio = 0.5\n",
    "num_tokens = test_input_ids.shape[1]\n",
    "num_mask = int(num_tokens * mask_ratio)\n",
    "mask_indices = torch.randperm(num_tokens)[:num_mask]\n",
    "\n",
    "masked_input_ids = test_input_ids.clone()\n",
    "masked_input_ids[0, mask_indices] = mask_token_id\n",
    "\n",
    "print(f\"Test input shape: {masked_input_ids.shape}\")\n",
    "print(f\"Number of masked tokens: {num_mask} / {num_tokens} ({mask_ratio*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dee1a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 225 hooks\n"
     ]
    }
   ],
   "source": [
    "original_activations = {}\n",
    "\n",
    "original_hooks = register_activation_hooks(original_model, original_activations)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = original_model(input_ids=masked_input_ids)\n",
    "\n",
    "remove_hooks(original_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c0724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.transformer.blocks.0.q_proj', 'model.transformer.blocks.0.k_proj', 'model.transformer.blocks.0.v_proj', 'model.transformer.blocks.0.attn_out', 'model.transformer.blocks.0.ff_proj', 'model.transformer.blocks.0.up_proj', 'model.transformer.blocks.0.ff_out', 'model.transformer.blocks.1.q_proj', 'model.transformer.blocks.1.k_proj', 'model.transformer.blocks.1.v_proj', 'model.transformer.blocks.1.attn_out', 'model.transformer.blocks.1.ff_proj', 'model.transformer.blocks.1.up_proj', 'model.transformer.blocks.1.ff_out', 'model.transformer.blocks.2.q_proj', 'model.transformer.blocks.2.k_proj', 'model.transformer.blocks.2.v_proj', 'model.transformer.blocks.2.attn_out', 'model.transformer.blocks.2.ff_proj', 'model.transformer.blocks.2.up_proj', 'model.transformer.blocks.2.ff_out', 'model.transformer.blocks.3.q_proj', 'model.transformer.blocks.3.k_proj', 'model.transformer.blocks.3.v_proj', 'model.transformer.blocks.3.attn_out', 'model.transformer.blocks.3.ff_proj', 'model.transformer.blocks.3.up_proj', 'model.transformer.blocks.3.ff_out', 'model.transformer.blocks.4.q_proj', 'model.transformer.blocks.4.k_proj', 'model.transformer.blocks.4.v_proj', 'model.transformer.blocks.4.attn_out', 'model.transformer.blocks.4.ff_proj', 'model.transformer.blocks.4.up_proj', 'model.transformer.blocks.4.ff_out', 'model.transformer.blocks.5.q_proj', 'model.transformer.blocks.5.k_proj', 'model.transformer.blocks.5.v_proj', 'model.transformer.blocks.5.attn_out', 'model.transformer.blocks.5.ff_proj', 'model.transformer.blocks.5.up_proj', 'model.transformer.blocks.5.ff_out', 'model.transformer.blocks.6.q_proj', 'model.transformer.blocks.6.k_proj', 'model.transformer.blocks.6.v_proj', 'model.transformer.blocks.6.attn_out', 'model.transformer.blocks.6.ff_proj', 'model.transformer.blocks.6.up_proj', 'model.transformer.blocks.6.ff_out', 'model.transformer.blocks.7.q_proj', 'model.transformer.blocks.7.k_proj', 'model.transformer.blocks.7.v_proj', 'model.transformer.blocks.7.attn_out', 'model.transformer.blocks.7.ff_proj', 'model.transformer.blocks.7.up_proj', 'model.transformer.blocks.7.ff_out', 'model.transformer.blocks.8.q_proj', 'model.transformer.blocks.8.k_proj', 'model.transformer.blocks.8.v_proj', 'model.transformer.blocks.8.attn_out', 'model.transformer.blocks.8.ff_proj', 'model.transformer.blocks.8.up_proj', 'model.transformer.blocks.8.ff_out', 'model.transformer.blocks.9.q_proj', 'model.transformer.blocks.9.k_proj', 'model.transformer.blocks.9.v_proj', 'model.transformer.blocks.9.attn_out', 'model.transformer.blocks.9.ff_proj', 'model.transformer.blocks.9.up_proj', 'model.transformer.blocks.9.ff_out', 'model.transformer.blocks.10.q_proj', 'model.transformer.blocks.10.k_proj', 'model.transformer.blocks.10.v_proj', 'model.transformer.blocks.10.attn_out', 'model.transformer.blocks.10.ff_proj', 'model.transformer.blocks.10.up_proj', 'model.transformer.blocks.10.ff_out', 'model.transformer.blocks.11.q_proj', 'model.transformer.blocks.11.k_proj', 'model.transformer.blocks.11.v_proj', 'model.transformer.blocks.11.attn_out', 'model.transformer.blocks.11.ff_proj', 'model.transformer.blocks.11.up_proj', 'model.transformer.blocks.11.ff_out', 'model.transformer.blocks.12.q_proj', 'model.transformer.blocks.12.k_proj', 'model.transformer.blocks.12.v_proj', 'model.transformer.blocks.12.attn_out', 'model.transformer.blocks.12.ff_proj', 'model.transformer.blocks.12.up_proj', 'model.transformer.blocks.12.ff_out', 'model.transformer.blocks.13.q_proj', 'model.transformer.blocks.13.k_proj', 'model.transformer.blocks.13.v_proj', 'model.transformer.blocks.13.attn_out', 'model.transformer.blocks.13.ff_proj', 'model.transformer.blocks.13.up_proj', 'model.transformer.blocks.13.ff_out', 'model.transformer.blocks.14.q_proj', 'model.transformer.blocks.14.k_proj', 'model.transformer.blocks.14.v_proj', 'model.transformer.blocks.14.attn_out', 'model.transformer.blocks.14.ff_proj', 'model.transformer.blocks.14.up_proj', 'model.transformer.blocks.14.ff_out', 'model.transformer.blocks.15.q_proj', 'model.transformer.blocks.15.k_proj', 'model.transformer.blocks.15.v_proj', 'model.transformer.blocks.15.attn_out', 'model.transformer.blocks.15.ff_proj', 'model.transformer.blocks.15.up_proj', 'model.transformer.blocks.15.ff_out', 'model.transformer.blocks.16.q_proj', 'model.transformer.blocks.16.k_proj', 'model.transformer.blocks.16.v_proj', 'model.transformer.blocks.16.attn_out', 'model.transformer.blocks.16.ff_proj', 'model.transformer.blocks.16.up_proj', 'model.transformer.blocks.16.ff_out', 'model.transformer.blocks.17.q_proj', 'model.transformer.blocks.17.k_proj', 'model.transformer.blocks.17.v_proj', 'model.transformer.blocks.17.attn_out', 'model.transformer.blocks.17.ff_proj', 'model.transformer.blocks.17.up_proj', 'model.transformer.blocks.17.ff_out', 'model.transformer.blocks.18.q_proj', 'model.transformer.blocks.18.k_proj', 'model.transformer.blocks.18.v_proj', 'model.transformer.blocks.18.attn_out', 'model.transformer.blocks.18.ff_proj', 'model.transformer.blocks.18.up_proj', 'model.transformer.blocks.18.ff_out', 'model.transformer.blocks.19.q_proj', 'model.transformer.blocks.19.k_proj', 'model.transformer.blocks.19.v_proj', 'model.transformer.blocks.19.attn_out', 'model.transformer.blocks.19.ff_proj', 'model.transformer.blocks.19.up_proj', 'model.transformer.blocks.19.ff_out', 'model.transformer.blocks.20.q_proj', 'model.transformer.blocks.20.k_proj', 'model.transformer.blocks.20.v_proj', 'model.transformer.blocks.20.attn_out', 'model.transformer.blocks.20.ff_proj', 'model.transformer.blocks.20.up_proj', 'model.transformer.blocks.20.ff_out', 'model.transformer.blocks.21.q_proj', 'model.transformer.blocks.21.k_proj', 'model.transformer.blocks.21.v_proj', 'model.transformer.blocks.21.attn_out', 'model.transformer.blocks.21.ff_proj', 'model.transformer.blocks.21.up_proj', 'model.transformer.blocks.21.ff_out', 'model.transformer.blocks.22.q_proj', 'model.transformer.blocks.22.k_proj', 'model.transformer.blocks.22.v_proj', 'model.transformer.blocks.22.attn_out', 'model.transformer.blocks.22.ff_proj', 'model.transformer.blocks.22.up_proj', 'model.transformer.blocks.22.ff_out', 'model.transformer.blocks.23.q_proj', 'model.transformer.blocks.23.k_proj', 'model.transformer.blocks.23.v_proj', 'model.transformer.blocks.23.attn_out', 'model.transformer.blocks.23.ff_proj', 'model.transformer.blocks.23.up_proj', 'model.transformer.blocks.23.ff_out', 'model.transformer.blocks.24.q_proj', 'model.transformer.blocks.24.k_proj', 'model.transformer.blocks.24.v_proj', 'model.transformer.blocks.24.attn_out', 'model.transformer.blocks.24.ff_proj', 'model.transformer.blocks.24.up_proj', 'model.transformer.blocks.24.ff_out', 'model.transformer.blocks.25.q_proj', 'model.transformer.blocks.25.k_proj', 'model.transformer.blocks.25.v_proj', 'model.transformer.blocks.25.attn_out', 'model.transformer.blocks.25.ff_proj', 'model.transformer.blocks.25.up_proj', 'model.transformer.blocks.25.ff_out', 'model.transformer.blocks.26.q_proj', 'model.transformer.blocks.26.k_proj', 'model.transformer.blocks.26.v_proj', 'model.transformer.blocks.26.attn_out', 'model.transformer.blocks.26.ff_proj', 'model.transformer.blocks.26.up_proj', 'model.transformer.blocks.26.ff_out', 'model.transformer.blocks.27.q_proj', 'model.transformer.blocks.27.k_proj', 'model.transformer.blocks.27.v_proj', 'model.transformer.blocks.27.attn_out', 'model.transformer.blocks.27.ff_proj', 'model.transformer.blocks.27.up_proj', 'model.transformer.blocks.27.ff_out', 'model.transformer.blocks.28.q_proj', 'model.transformer.blocks.28.k_proj', 'model.transformer.blocks.28.v_proj', 'model.transformer.blocks.28.attn_out', 'model.transformer.blocks.28.ff_proj', 'model.transformer.blocks.28.up_proj', 'model.transformer.blocks.28.ff_out', 'model.transformer.blocks.29.q_proj', 'model.transformer.blocks.29.k_proj', 'model.transformer.blocks.29.v_proj', 'model.transformer.blocks.29.attn_out', 'model.transformer.blocks.29.ff_proj', 'model.transformer.blocks.29.up_proj', 'model.transformer.blocks.29.ff_out', 'model.transformer.blocks.30.q_proj', 'model.transformer.blocks.30.k_proj', 'model.transformer.blocks.30.v_proj', 'model.transformer.blocks.30.attn_out', 'model.transformer.blocks.30.ff_proj', 'model.transformer.blocks.30.up_proj', 'model.transformer.blocks.30.ff_out', 'model.transformer.blocks.31.q_proj', 'model.transformer.blocks.31.k_proj', 'model.transformer.blocks.31.v_proj', 'model.transformer.blocks.31.attn_out', 'model.transformer.blocks.31.ff_proj', 'model.transformer.blocks.31.up_proj', 'model.transformer.blocks.31.ff_out', 'model.transformer.ff_out'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f119f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "joint_layer_activations = {}\n",
    "\n",
    "for key in original_activations:\n",
    "    if not re.search(r'\\d+', key) or key.find('ff_out') != -1:\n",
    "        continue\n",
    "    layer = key.split('.')[3]\n",
    "    if layer not in joint_layer_activations:\n",
    "        joint_layer_activations[layer] = original_activations[key]\n",
    "    else:\n",
    "        joint_layer_activations[layer] = torch.cat([joint_layer_activations[layer], original_activations[key]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ff631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584f1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
